#cloud-config
coreos:
  update:
    reboot-strategy: "off"
  flannel:
    interface: $private_ipv4
    etcd_endpoints: {{ .EtcdEndpoints }}
    etcd_cafile: /etc/kubernetes/ssl/ca.pem
    etcd_certfile: /etc/kubernetes/ssl/etcd-client.pem
    etcd_keyfile: /etc/kubernetes/ssl/etcd-client-key.pem

  units:
{{if .Experimental.AwsEnvironment.Enabled}}
    - name: set-aws-environment.service
      enable: true
      command: start
      runtime: true
      content: |
        [Unit]
        Description=Set AWS environment variables in /etc/aws-environment
        After=network-online.target

        [Service]
        Type=oneshot
        RemainAfterExit=true
        ExecStartPre=/bin/touch /etc/aws-environment
        ExecStart=/usr/bin/rkt run \
          --uuid-file-save=/var/run/coreos/cfn-init.uuid \
          --volume=dns,kind=host,source=/etc/resolv.conf,readOnly=true --mount volume=dns,target=/etc/resolv.conf \
          --volume=awsenv,kind=host,source=/etc/aws-environment,readOnly=false --mount volume=awsenv,target=/etc/aws-environment \
          --net=host \
          --trust-keys-from-https \
          {{.AWSCliImageRepo}}:{{.AWSCliTag}} -- cfn-init -v \
              --region {{.Region}} \
              --resource LaunchConfigurationController \
              --stack {{.ClusterName}}
{{end}}
    - name: docker.service
      drop-ins:
{{if .Experimental.EphemeralImageStorage.Enabled}}
        - name: 10-docker-mount.conf
          content: |
            [Unit]
            After=var-lib-docker.mount
            Wants=var-lib-docker.mount
{{end}}
        - name: 40-flannel.conf
          content: |
            [Unit]
            Wants=flanneld.service
            [Service]
            EnvironmentFile=/etc/kubernetes/cni/docker_opts_cni.env
            ExecStartPre=/usr/bin/systemctl is-active flanneld.service

    - name: flanneld.service
      drop-ins:
        - name: 10-etcd.conf
          content: |
            [Service]
            Environment="ETCD_SSL_DIR=/etc/kubernetes/ssl"
            ExecStartPre=/opt/bin/decrypt-tls-assets
            ExecStartPre=/usr/bin/etcdctl \
            --ca-file=/etc/kubernetes/ssl/ca.pem \
            --cert-file=/etc/kubernetes/ssl/etcd-client.pem \
            --key-file=/etc/kubernetes/ssl/etcd-client-key.pem \
            --endpoints="{{.EtcdEndpoints}}" \
            set /coreos.com/network/config '{"Network" : "{{.PodCIDR}}", "Backend" : {"Type" : "vxlan"}}'
            TimeoutStartSec=120

    - name: kubelet.service
      command: start
      runtime: true
      content: |
        [Unit]
        Wants=flanneld.service
        [Service]
        Environment=KUBELET_VERSION={{.K8sVer}}
        Environment=KUBELET_ACI={{.HyperkubeImageRepo}}
        Environment="RKT_OPTS=--volume dns,kind=host,source=/etc/resolv.conf \
        --set-env=ETCD_CA_CERT_FILE=/etc/kubernetes/ssl/ca.pem \
        --set-env=ETCD_CERT_FILE=/etc/kubernetes/ssl/etcd-client.pem \
        --set-env=ETCD_KEY_FILE=/etc/kubernetes/ssl/etcd-client-key.pem \
        --mount volume=dns,target=/etc/resolv.conf \
        --volume rkt,kind=host,source=/opt/bin/host-rkt \
        --mount volume=rkt,target=/usr/bin/rkt \
        --volume var-lib-rkt,kind=host,source=/var/lib/rkt \
        --mount volume=var-lib-rkt,target=/var/lib/rkt \
        --volume var-lib-cni,kind=host,source=/var/lib/cni \
        --mount volume=var-lib-cni,target=/var/lib/cni \
        --volume stage,kind=host,source=/tmp \
        --mount volume=stage,target=/tmp \
        --volume var-log,kind=host,source=/var/log \
        --mount volume=var-log,target=/var/log"
        ExecStartPre=/usr/bin/systemctl is-active flanneld.service
        ExecStartPre=/usr/bin/mkdir -p /var/lib/cni
        ExecStartPre=/usr/bin/mkdir -p /var/log/containers
        ExecStart=/usr/lib/coreos/kubelet-wrapper \
        --api-servers=http://localhost:8080 \
        --cni-conf-dir=/etc/kubernetes/cni/net.d \
        --network-plugin={{.K8sNetworkPlugin}} \
        --container-runtime={{.ContainerRuntime}} \
        --rkt-path=/usr/bin/rkt \
        --rkt-stage1-image=coreos.com/rkt/stage1-coreos \
        --register-schedulable=false \
        --allow-privileged=true \
        --pod-manifest-path=/etc/kubernetes/manifests \
        --cluster_dns={{.DNSServiceIP}} \
        --cluster_domain=cluster.local \
        --cloud-provider=aws
        Restart=always
        RestartSec=10

        [Install]
        WantedBy=multi-user.target

{{ if eq .ContainerRuntime "rkt" }}
    - name: rkt-api.service
      enable: true
      content: |
        [Unit]
        Before=kubelet.service
        [Service]
        ExecStart=/usr/bin/rkt api-service
        Restart=always
        RestartSec=10
        [Install]
        RequiredBy=kubelet.service

    - name: load-rkt-stage1.service
      enable: true
      content: |
        [Unit]
        Description=Load rkt stage1 images
        Documentation=http://github.com/coreos/rkt
        Requires=network-online.target
        After=network-online.target
        Before=rkt-api.service
        [Service]
        Type=oneshot
        RemainAfterExit=yes
        ExecStart=/usr/bin/rkt fetch /usr/lib/rkt/stage1-images/stage1-coreos.aci /usr/lib/rkt/stage1-images/stage1-fly.aci  --insecure-options=image
        [Install]
        RequiredBy=rkt-api.service
{{ end }}

{{ if .UseCalico }}
    - name: calico-node.service
      command: start
      runtime: true
      content: |
        [Unit]
        Description=Calico per-host agent
        Requires=network-online.target
        After=network-online.target

        [Service]
        Slice=machine.slice
        Environment=CALICO_DISABLE_FILE_LOGGING=true
        Environment=HOSTNAME=$private_ipv4
        Environment=IP=$private_ipv4
        Environment=FELIX_FELIXHOSTNAME=$private_ipv4
        Environment=CALICO_NETWORKING=false
        Environment=NO_DEFAULT_POOLS=true
        Environment=ETCD_SCHEME=https
        Environment=ETCD_CA_CERT_FILE=/etc/etcd2/ssl/ca.pem
        Environment=ETCD_CERT_FILE=/etc/etcd2/ssl/etcd-client.pem
        Environment=ETCD_KEY_FILE=/etc/etcd2/ssl/etcd-client-key.pem
        Environment=ETCD_ENDPOINTS={{ .EtcdEndpoints }}
        ExecStart=/usr/bin/rkt run --inherit-env --stage1-from-dir=stage1-fly.aci \
        --volume=modules,kind=host,source=/lib/modules,readOnly=false \
        --mount=volume=modules,target=/lib/modules \
        --volume=dns,kind=host,source=/etc/resolv.conf,readOnly=true \
        --mount=volume=dns,target=/etc/resolv.conf \
        --volume=ssl,kind=host,source=/etc/kubernetes/ssl,readOnly=true \
        --mount=volume=ssl,target=/etc/etcd2/ssl \
        --trust-keys-from-https quay.io/calico/node:v0.22.0
        KillMode=mixed
        Restart=always
        TimeoutStartSec=0

        [Install]
        WantedBy=multi-user.target
{{ end }}

    - name: install-kube-system.service
      command: start
      runtime: true
      content: |
        [Unit]
        Wants=kubelet.service docker.service

        [Service]
        Type=simple
        StartLimitInterval=0
        RestartSec=10
        Restart=on-failure
        ExecStartPre=/usr/bin/systemctl is-active kubelet.service
        ExecStartPre=/usr/bin/systemctl is-active docker.service
        ExecStartPre=/usr/bin/curl http://127.0.0.1:8080/version
        ExecStart=/opt/bin/install-kube-system

{{ if .UseCalico }}
    - name: install-calico-system.service
      command: start
      runtime: true
      content: |
        [Unit]
        Wants=kubelet.service docker.service

        [Service]
        Type=simple
        StartLimitInterval=0
        RestartSec=10
        Restart=on-failure
        ExecStartPre=/usr/bin/systemctl is-active kubelet.service
        ExecStartPre=/usr/bin/systemctl is-active docker.service
        ExecStartPre=/usr/bin/curl http://127.0.0.1:8080/version
        ExecStart=/opt/bin/install-calico-system
{{ end }}
{{ if $.ElasticFileSystemID }}
    - name: rpc-statd.service
      command: start
      enable: true
    - name: efs.service
      command: start
      content: |
        [Unit]
        After=network-online.target
        Before=kubelet.service
        [Service]
        Type=oneshot
        ExecStartPre=-/usr/bin/mkdir -p /efs
        ExecStart=/bin/sh -c 'grep -qs /efs /proc/mounts || /usr/bin/mount -t nfs4 -o nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2 $(/usr/bin/curl -s http://169.254.169.254/latest/meta-data/placement/availability-zone).{{ $.ElasticFileSystemID }}.efs.{{ $.Region }}.amazonaws.com:/ /efs'
        ExecStop=/usr/bin/umount /efs
        RemainAfterExit=yes
        [Install]
        WantedBy=kubelet.service
{{ end }}
    - name: kube-node-taint-and-uncordon.service
      command: start
      runtime: true
      content: |
        [Unit]
        Description=Taint this kubernetes node with its role and then uncordon it
        Wants=kubelet.service
        After=kubelet.service
        Before=cfn-signal.service

        [Service]
        Type=simple
        StartLimitInterval=0
        RestartSec=10
        Restart=on-failure
        ExecStartPre=/usr/bin/systemctl is-active kubelet.service
        ExecStartPre=/usr/bin/bash -c "while sleep 1; do if /usr/bin/curl -s -m 20 -f  http://127.0.0.1:8080/healthz > /dev/null &&  /usr/bin/curl -s -m 20 -f  http://127.0.0.1:10252/healthz > /dev/null && /usr/bin/curl -s -m 20 -f  http://127.0.0.1:10251/healthz > /dev/null &&  /usr/bin/curl --insecure -s -m 20 -f  https://127.0.0.1:10250/healthz > /dev/null ; then break ; fi;  done"
        ExecStart=/opt/bin/taint-and-uncordon
{{if .Experimental.WaitSignal.Enabled}}
    - name: cfn-signal.service
      command: start
      content: |
        [Unit]
        Wants=kubelet.service docker.service
        After=kubelet.service

        [Service]
        Type=oneshot
        ExecStartPre=/usr/bin/bash -c "while sleep 1; do if /usr/bin/curl -s -m 20 -f  http://127.0.0.1:8080/healthz > /dev/null &&  /usr/bin/curl -s -m 20 -f  http://127.0.0.1:10252/healthz > /dev/null && /usr/bin/curl -s -m 20 -f  http://127.0.0.1:10251/healthz > /dev/null &&  /usr/bin/curl --insecure -s -m 20 -f  https://127.0.0.1:10250/healthz > /dev/null ; then break ; fi;  done"
        {{ if .UseCalico }}
        ExecStartPre=/usr/bin/systemctl is-active calico-node
        {{ end }}
        ExecStart=/usr/bin/rkt run \
          --uuid-file-save=/var/run/coreos/cfn-signal.uuid \
          --volume=dns,kind=host,source=/etc/resolv.conf,readOnly=true --mount volume=dns,target=/etc/resolv.conf \
          --net=host \
          --trust-keys-from-https \
          quay.io/coreos/awscli:edge -- cfn-signal -e 0 \
              --region {{.Region}} \
              --resource AutoScaleController \
              --stack {{.ClusterName}}
{{end}}
{{if .Experimental.AwsNodeLabels.Enabled }}
    - name: kube-node-label.service
      enable: true
      command: start
      runtime: true
      content: |
        [Unit]
        Description=Label this kubernetes node with additional AWS parameters
        After=kubelet.service
        Before=cfn-signal.service

        [Service]
        Type=oneshot
        ExecStop=/bin/true
        RemainAfterExit=true
        ExecStartPre=/bin/sh -c "/usr/bin/systemctl set-environment INSTANCE_ID=$(/usr/bin/curl -s http://169.254.169.254/latest/meta-data/instance-id)"
        ExecStartPre=/bin/sh -c "/usr/bin/systemctl set-environment SECURITY_GROUPS=\"$(/usr/bin/curl -s http://169.254.169.254/latest/meta-data/security-groups | tr '\n' ',')\""
        ExecStartPre=/bin/sh -c "/usr/bin/systemctl set-environment AUTOSCALINGGROUP=\"$(/usr/bin/docker run --rm --net=host \
          {{.AWSCliImageRepo}}:{{.AWSCliTag}} aws \
          autoscaling describe-auto-scaling-instances \
          --instance-ids ${INSTANCE_ID} --region {{.Region}} \
          --query 'AutoScalingInstances[].AutoScalingGroupName' --output text)\""
        ExecStartPre=/bin/sh -c "/usr/bin/systemctl set-environment \
          LAUNCHCONFIGURATION=\"$(/usr/bin/docker run --rm --net=host \
          {{.AWSCliImageRepo}}:{{.AWSCliTag}} \
          aws autoscaling describe-auto-scaling-groups \
          --auto-scaling-group-name $AUTOSCALINGGROUP --region {{.Region}} \
          --query 'AutoScalingGroups[].LaunchConfigurationName' --output text)\""
        ExecStart=/bin/sh -c "/usr/bin/curl \
          --request PATCH \
          -H 'Content-Type: application/strategic-merge-patch+json' \
          -d'{ \
          \"metadata\": { \
            \"labels\": { \
              \"kube-aws.coreos.com/autoscalinggroup\": \"${AUTOSCALINGGROUP}\", \
              \"kube-aws.coreos.com/launchconfiguration\": \"${LAUNCHCONFIGURATION}\" \
            }, \
            \"annotations\": { \
              \"kube-aws.coreos.com/securitygroups\": \"${SECURITY_GROUPS}\" \
            } \
          } \
          }\"' \
          http://localhost:8080/api/v1/nodes/$(hostname)"
{{end}}

{{if .Experimental.EphemeralImageStorage.Enabled}}
    - name: format-ephemeral.service
      command: start
      content: |
        [Unit]
        Description=Formats the ephemeral drive
        ConditionFirstBoot=yes
        After=dev-{{.Experimental.EphemeralImageStorage.Disk}}.device
        Requires=dev-{{.Experimental.EphemeralImageStorage.Disk}}.device
        [Service]
        Type=oneshot
        RemainAfterExit=yes
        ExecStart=/usr/sbin/wipefs -f /dev/{{.Experimental.EphemeralImageStorage.Disk}}
        ExecStart=/usr/sbin/mkfs.{{.Experimental.EphemeralImageStorage.Filesystem}} -f /dev/{{.Experimental.EphemeralImageStorage.Disk}}
    - name: var-lib-docker.mount
      command: start
      content: |
        [Unit]
        Description=Mount ephemeral to /var/lib/docker
        Requires=format-ephemeral.service
        After=format-ephemeral.service
        [Mount]
        What=/dev/{{.Experimental.EphemeralImageStorage.Disk}}
{{if eq .ContainerRuntime "docker"}}
        Where=/var/lib/docker
{{else if eq .ContainerRuntime "rkt"}}
        Where=/var/lib/rkt
{{end}}
        Type={{.Experimental.EphemeralImageStorage.Filesystem}}
{{end}}

{{if .SSHAuthorizedKeys}}
ssh_authorized_keys:
  {{range $sshkey := .SSHAuthorizedKeys}}
  - {{$sshkey}}
  {{end}}
{{end}}

write_files:
  - path: /opt/bin/install-kube-system
    permissions: 0700
    owner: root:root
    content: |
      #!/bin/bash -e

      post_yaml() {
          /usr/bin/curl  -H "Content-Type: application/yaml" -XPOST \
          --data-binary "$1" "$2"
      }

      mfdir=/srv/kubernetes/manifests
      for manifest in $mfdir/{kube-dns-de,kube-dns-autoscaler-de,heapster-de}.yaml;do
          post_yaml "@$mfdir/$manifest" \
          "http://127.0.0.1:8080/apis/extensions/v1beta1/namespaces/kube-system/deployments"
      done

      post_yaml "@$mfdir/kube-dashboard-rc.yaml" \
      "http://127.0.0.1:8080/api/v1/namespaces/kube-system/replicationcontrollers"

      for manifest in {kube-dns,heapster,kube-dashboard}-svc.yaml;do
          post_yaml "@$mfdir/$manifest" \
          "http://127.0.0.1:8080/api/v1/namespaces/kube-system/services"
      done

  - path: /etc/kubernetes/cni/docker_opts_cni.env
    content: |
      DOCKER_OPT_BIP=""
      DOCKER_OPT_IPMASQ=""

  - path: /opt/bin/host-rkt
    permissions: 0755
    owner: root:root
    content: |
      #!/bin/sh
      # This is bind mounted into the kubelet rootfs and all rkt shell-outs go
      # through this rkt wrapper. It essentially enters the host mount namespace
      # (which it is already in) only for the purpose of breaking out of the chroot
      # before calling rkt. It makes things like rkt gc work and avoids bind mounting
      # in certain rkt filesystem dependancies into the kubelet rootfs. This can
      # eventually be obviated when the write-api stuff gets upstream and rkt gc is
      # through the api-server. Related issue:
      # https://github.com/coreos/rkt/issues/2878
      exec nsenter -m -u -i -n -p -t 1 -- /usr/bin/rkt "$@"

{{ if .UseCalico }}
  - path: /opt/bin/install-calico-system
    permissions: 0700
    owner: root:root
    content: |
      #!/bin/bash -e
      /usr/bin/curl -H "Content-Type: application/json" -XPOST --data-binary @"/srv/kubernetes/manifests/calico-system.json" "http://127.0.0.1:8080/api/v1/namespaces"

      /usr/bin/cp /srv/kubernetes/manifests/calico-policy-controller.yaml /etc/kubernetes/manifests
{{ end }}

  - path: /opt/bin/decrypt-tls-assets
    owner: root:root
    permissions: 0700
    content: |
      #!/bin/bash -e

      sudo rkt run \
        --volume=ssl,kind=host,source=/etc/kubernetes/ssl,readOnly=false \
        --mount=volume=ssl,target=/etc/kubernetes/ssl \
        --uuid-file-save=/var/run/coreos/decrypt-tls-assets.uuid \
        --volume=dns,kind=host,source=/etc/resolv.conf,readOnly=true --mount volume=dns,target=/etc/resolv.conf \
        --net=host \
        --trust-keys-from-https \
        {{.AWSCliImageRepo}}:{{.AWSCliTag}} --exec=/bin/bash -- \
          -ec \
          'echo decrypting tls assets
           shopt -s nullglob
           for encKey in /etc/kubernetes/ssl/*.pem.enc; do
             echo decrypting $encKey
             /usr/bin/aws \
               --region {{.Region}} kms decrypt \
               --ciphertext-blob fileb://$encKey \
               --output text \
               --query Plaintext \
             | base64 --decode ${encKey%.enc}
           done;
           echo done.'

      sudo rkt rm --uuid-file=/var/run/coreos/decrypt-tls-assets.uuid

  - path: /opt/bin/taint-and-uncordon
    owner: root:root
    permissions: 0700
    content: |
      #!/bin/bash -e

      hostname=$(hostname)

      sudo rkt run \
        --volume=kube,kind=host,source=/etc/kubernetes,readOnly=true \
        --mount=volume=kube,target=/etc/kubernetes \
        --uuid-file-save=/var/run/coreos/taint-and-uncordon.uuid \
        --volume=dns,kind=host,source=/etc/resolv.conf,readOnly=true --mount volume=dns,target=/etc/resolv.conf \
        --net=host \
        --trust-keys-from-https \
        {{.HyperkubeImageRepo}}:{{.K8sVer}} --exec=/bin/bash -- \
          -vxec \
          'echo tainting this node
           hostname="'${hostname}'"
           kubectl="/kubectl --server=http://127.0.0.1:8080"
           taint="$kubectl taint node $hostname"
           $taint "node.alpha.kubernetes.io/role:master:NoSchedule"
           echo done.
           echo uncordoning this node
           $kubectl uncordon $hostname
           echo done.'

      echo cleaning pod resources.

      sudo rkt rm --uuid-file=/var/run/coreos/taint-and-uncordon.uuid

      echo done.

  - path: /etc/kubernetes/manifests/kube-proxy.yaml
    content: |
        apiVersion: v1
        kind: Pod
        metadata:
          name: kube-proxy
          namespace: kube-system
          annotations:
            rkt.alpha.kubernetes.io/stage1-name-override: coreos.com/rkt/stage1-fly
        spec:
          hostNetwork: true
          containers:
          - name: kube-proxy
            image: {{.HyperkubeImageRepo}}:{{.K8sVer}}
            command:
            - /hyperkube
            - proxy
            - --master=http://127.0.0.1:8080
            securityContext:
              privileged: true
            volumeMounts:
            - mountPath: /etc/ssl/certs
              name: ssl-certs-host
              readOnly: true
            - mountPath: /var/run/dbus
              name: dbus
              readOnly: false
          volumes:
          - hostPath:
              path: /usr/share/ca-certificates
            name: ssl-certs-host
          - hostPath:
              path: /var/run/dbus
            name: dbus

  - path: /etc/kubernetes/manifests/kube-apiserver.yaml
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-apiserver
        namespace: kube-system
      spec:
        hostNetwork: true
        containers:
        - name: kube-apiserver
          image: {{.HyperkubeImageRepo}}:{{.K8sVer}}
          command:
          - /hyperkube
          - apiserver
          - --bind-address=0.0.0.0
          - --etcd-servers={{.EtcdEndpoints}}
          - --etcd-cafile=/etc/kubernetes/ssl/ca.pem
          - --etcd-certfile=/etc/kubernetes/ssl/etcd-client.pem
          - --etcd-keyfile=/etc/kubernetes/ssl/etcd-client-key.pem
          - --allow-privileged=true
          - --service-cluster-ip-range={{.ServiceCIDR}}
          - --secure-port=443
          {{if .Experimental.AuditLog.Enabled}}
          - --audit-log-maxage={{.Experimental.AuditLog.MaxAge}}
          - --audit-log-path={{.Experimental.AuditLog.LogPath}}
          {{ end }}
          {{if .Experimental.Plugins.Rbac.Enabled}}
          - --authorization-mode=RBAC
          - --authorization-rbac-super-user=kube-admin
          {{ end }}
          - --advertise-address=$private_ipv4
          - --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota
          - --anonymous-auth=false
          - --tls-cert-file=/etc/kubernetes/ssl/apiserver.pem
          - --tls-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem
          - --client-ca-file=/etc/kubernetes/ssl/ca.pem
          - --service-account-key-file=/etc/kubernetes/ssl/apiserver-key.pem
          - --runtime-config=extensions/v1beta1/networkpolicies=true,batch/v2alpha1{{if .Experimental.Plugins.Rbac.Enabled}},rbac.authorization.k8s.io/v1alpha1=true{{ end }}
          - --cloud-provider=aws
          livenessProbe:
            httpGet:
              host: 127.0.0.1
              port: 8080
              path: /healthz
            initialDelaySeconds: 15
            timeoutSeconds: 15
          ports:
          - containerPort: 443
            hostPort: 443
            name: https
          - containerPort: 8080
            hostPort: 8080
            name: local
          volumeMounts:
          - mountPath: /etc/kubernetes/ssl
            name: ssl-certs-kubernetes
            readOnly: true
          - mountPath: /etc/ssl/certs
            name: ssl-certs-host
            readOnly: true
        volumes:
        - hostPath:
            path: /etc/kubernetes/ssl
          name: ssl-certs-kubernetes
        - hostPath:
            path: /usr/share/ca-certificates
          name: ssl-certs-host

  - path: /etc/kubernetes/manifests/kube-controller-manager.yaml
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-controller-manager
        namespace: kube-system
      spec:
        containers:
        - name: kube-controller-manager
          image: {{.HyperkubeImageRepo}}:{{.K8sVer}}
          command:
          - /hyperkube
          - controller-manager
          - --master=http://127.0.0.1:8080
          - --leader-elect=true
          - --service-account-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem
          - --root-ca-file=/etc/kubernetes/ssl/ca.pem
          - --cloud-provider=aws
          resources:
            requests:
              cpu: 200m
          livenessProbe:
            httpGet:
              host: 127.0.0.1
              path: /healthz
              port: 10252
            initialDelaySeconds: 15
            timeoutSeconds: 15
          volumeMounts:
          - mountPath: /etc/kubernetes/ssl
            name: ssl-certs-kubernetes
            readOnly: true
          - mountPath: /etc/ssl/certs
            name: ssl-certs-host
            readOnly: true
        hostNetwork: true
        volumes:
        - hostPath:
            path: /etc/kubernetes/ssl
          name: ssl-certs-kubernetes
        - hostPath:
            path: /usr/share/ca-certificates
          name: ssl-certs-host

  - path: /etc/kubernetes/manifests/kube-scheduler.yaml
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: kube-scheduler
        namespace: kube-system
      spec:
        hostNetwork: true
        containers:
        - name: kube-scheduler
          image: {{.HyperkubeImageRepo}}:{{.K8sVer}}
          command:
          - /hyperkube
          - scheduler
          - --master=http://127.0.0.1:8080
          - --leader-elect=true
          resources:
            requests:
              cpu: 100m
          livenessProbe:
            httpGet:
              host: 127.0.0.1
              path: /healthz
              port: 10251
            initialDelaySeconds: 15
            timeoutSeconds: 15

{{ if .UseCalico }}
  - path: /srv/kubernetes/manifests/calico-policy-controller.yaml
    content: |
      apiVersion: v1
      kind: Pod
      metadata:
        name: calico-policy-controller
        namespace: calico-system
      spec:
        hostNetwork: true
        containers:
          # The Calico policy controller.
          - name: kube-policy-controller
            image: calico/kube-policy-controller:v0.3.0
            env:
              - name: ETCD_SCHEME
                value: https
              - name: ETCD_CA_CERT_FILE
                value: "/etc/etcd2/ssl/ca.pem"
              - name: ETCD_CERT_FILE
                value: "/etc/etcd2/ssl/etcd-client.pem"
              - name: ETCD_KEY_FILE
                value: "/etc/etcd2/ssl/etcd-client-key.pem"
              - name: ETCD_ENDPOINTS
                value: "{{ .EtcdEndpoints }}"
              - name: K8S_API
                value: "http://127.0.0.1:8080"
              - name: LEADER_ELECTION
                value: "true"
            volumeMounts:
            - mountPath: /etc/etcd2/ssl
              name: ssl
          # Leader election container used by the policy controller.
          - name: leader-elector
            image: gcr.io/google_containers/leader-elector:0.5
            imagePullPolicy: IfNotPresent
            args:
              - "--election=calico-policy-election"
              - "--election-namespace=calico-system"
              - "--http=127.0.0.1:4040"
        volumes:
        - name: ssl
          hostPath:
            path: /etc/kubernetes/ssl
{{ end }}

{{ if .UseCalico }}
  - path: /srv/kubernetes/manifests/calico-system.json
    content: |
        {
          "apiVersion": "v1",
          "kind": "Namespace",
          "metadata": {
            "name": "calico-system"
          }
        }
{{ end }}

  - path: /srv/kubernetes/manifests/kube-dns-autoscaler-de.yaml
    content: |
        apiVersion: extensions/v1beta1
        kind: Deployment
        metadata:
          name: kube-dns-autoscaler
          namespace: kube-system
          labels:
            k8s-app: kube-dns-autoscaler
            kubernetes.io/cluster-service: "true"
        spec:
          template:
            metadata:
              labels:
                k8s-app: kube-dns-autoscaler
              annotations:
                scheduler.alpha.kubernetes.io/critical-pod: ''
                scheduler.alpha.kubernetes.io/tolerations: '[{"key":"CriticalAddonsOnly", "operator":"Exists"}]'
            spec:
              containers:
              - name: autoscaler
                image: gcr.io/google_containers/cluster-proportional-autoscaler-amd64:1.0.0
                resources:
                    requests:
                        cpu: "20m"
                        memory: "10Mi"
                command:
                  - /cluster-proportional-autoscaler
                  - --namespace=kube-system
                  - --configmap=kube-dns-autoscaler
                  - --mode=linear
                  - --target=Deployment/kube-dns
                  - --default-params={"linear":{"coresPerReplica":256,"nodesPerReplica":16,"min":2}}
                  - --logtostderr=true
                  - --v=2

  - path: /srv/kubernetes/manifests/kube-dns-de.yaml
    content: |
        apiVersion: extensions/v1beta1
        kind: Deployment
        metadata:
          name: kube-dns
          namespace: kube-system
          labels:
            k8s-app: kube-dns
            kubernetes.io/cluster-service: "true"
        spec:
          # replicas: not specified here:
          # 1. In order to make Addon Manager do not reconcile this replicas parameter.
          # 2. Default is 1.
          # 3. Will be tuned in real time if DNS horizontal auto-scaling is turned on.
          strategy:
            rollingUpdate:
              maxSurge: 10%
              maxUnavailable: 0
          selector:
            matchLabels:
              k8s-app: kube-dns
          template:
            metadata:
              labels:
                k8s-app: kube-dns
              annotations:
                scheduler.alpha.kubernetes.io/critical-pod: ''
                scheduler.alpha.kubernetes.io/tolerations: '[{"key":"CriticalAddonsOnly", "operator":"Exists"}]'
            spec:
              containers:
              - name: kubedns
                image: gcr.io/google_containers/kubedns-amd64:1.9
                resources:
                  limits:
                    memory: 170Mi
                  requests:
                    cpu: 100m
                    memory: 70Mi
                livenessProbe:
                  httpGet:
                    path: /healthz-kubedns
                    port: 8080
                    scheme: HTTP
                  initialDelaySeconds: 60
                  timeoutSeconds: 5
                  successThreshold: 1
                  failureThreshold: 5
                readinessProbe:
                  httpGet:
                    path: /readiness
                    port: 8081
                    scheme: HTTP
                  initialDelaySeconds: 3
                  timeoutSeconds: 5
                args:
                - --domain=cluster.local.
                - --dns-port=10053
                - --config-map=kube-dns
                # This should be set to v=2 only after the new image (cut from 1.5) has
                # been released, otherwise we will flood the logs.
                - --v=2
                env:
                - name: PROMETHEUS_PORT
                  value: "10055"
                ports:
                - containerPort: 10053
                  name: dns-local
                  protocol: UDP
                - containerPort: 10053
                  name: dns-tcp-local
                  protocol: TCP
                - containerPort: 10055
                  name: metrics
                  protocol: TCP
              - name: dnsmasq
                image: gcr.io/google_containers/kube-dnsmasq-amd64:1.4
                livenessProbe:
                  httpGet:
                    path: /healthz-dnsmasq
                    port: 8080
                    scheme: HTTP
                  initialDelaySeconds: 60
                  timeoutSeconds: 5
                  successThreshold: 1
                  failureThreshold: 5
                args:
                - --cache-size=1000
                - --no-resolv
                - --server=127.0.0.1#10053
                - --log-facility=-
                ports:
                - containerPort: 53
                  name: dns
                  protocol: UDP
                - containerPort: 53
                  name: dns-tcp
                  protocol: TCP
                # see: https://github.com/kubernetes/kubernetes/issues/29055 for details
                resources:
                  requests:
                    cpu: 150m
                    memory: 10Mi
              - name: dnsmasq-metrics
                image: gcr.io/google_containers/dnsmasq-metrics-amd64:1.0
                livenessProbe:
                  httpGet:
                    path: /metrics
                    port: 10054
                    scheme: HTTP
                  initialDelaySeconds: 60
                  timeoutSeconds: 5
                  successThreshold: 1
                  failureThreshold: 5
                args:
                - --v=2
                - --logtostderr
                ports:
                - containerPort: 10054
                  name: metrics
                  protocol: TCP
                resources:
                  requests:
                    memory: 10Mi
              - name: healthz
                image: gcr.io/google_containers/exechealthz-amd64:1.2
                resources:
                  limits:
                    memory: 50Mi
                  requests:
                    cpu: 10m
                    memory: 50Mi
                args:
                - --cmd=nslookup kubernetes.default.svc.cluster.local 127.0.0.1 >/dev/null
                - --url=/healthz-dnsmasq
                - --cmd=nslookup kubernetes.default.svc.cluster.local 127.0.0.1:10053 >/dev/null
                - --url=/healthz-kubedns
                - --port=8080
                - --quiet
                ports:
                - containerPort: 8080
                  protocol: TCP
              dnsPolicy: Default

  - path: /srv/kubernetes/manifests/kube-dns-svc.yaml
    content: |
        apiVersion: v1
        kind: Service
        metadata:
          name: kube-dns
          namespace: kube-system
          labels:
            k8s-app: kube-dns
            kubernetes.io/cluster-service: "true"
            kubernetes.io/name: "KubeDNS"
        spec:
          selector:
            k8s-app: kube-dns
          clusterIP: {{.DNSServiceIP}}
          ports:
          - name: dns
            port: 53
            protocol: UDP
          - name: dns-tcp
            port: 53
            protocol: TCP

  - path: /srv/kubernetes/manifests/heapster-de.yaml
    content: |
        apiVersion: extensions/v1beta1
        kind: Deployment
        metadata:
          name: heapster-v1.2.0
          namespace: kube-system
          labels:
            k8s-app: heapster
            kubernetes.io/cluster-service: "true"
            version: v1.2.0
        spec:
          replicas: 1
          selector:
            matchLabels:
              k8s-app: heapster
              version: v1.2.0
          template:
            metadata:
              labels:
                k8s-app: heapster
                version: v1.2.0
              annotations:
                scheduler.alpha.kubernetes.io/critical-pod: ''
                scheduler.alpha.kubernetes.io/tolerations: '[{"key":"CriticalAddonsOnly", "operator":"Exists"}]'
            spec:
              containers:
                - image: gcr.io/google_containers/heapster:v1.2.0
                  name: heapster
                  livenessProbe:
                    httpGet:
                      path: /healthz
                      port: 8082
                      scheme: HTTP
                    initialDelaySeconds: 180
                    timeoutSeconds: 5
                  resources:
                    limits:
                      cpu: 80m
                      memory: 200Mi
                    requests:
                      cpu: 80m
                      memory: 200Mi
                  command:
                    - /heapster
                    - --source=kubernetes.summary_api:''
                - image: gcr.io/google_containers/addon-resizer:1.6
                  name: heapster-nanny
                  resources:
                    limits:
                      cpu: 50m
                      memory: 90Mi
                    requests:
                      cpu: 50m
                      memory: 90Mi
                  env:
                    - name: MY_POD_NAME
                      valueFrom:
                        fieldRef:
                          fieldPath: metadata.name
                    - name: MY_POD_NAMESPACE
                      valueFrom:
                        fieldRef:
                          fieldPath: metadata.namespace
                  command:
                    - /pod_nanny
                    - --cpu=80m
                    - --extra-cpu=4m
                    - --memory=200Mi
                    - --extra-memory=4Mi
                    - --threshold=5
                    - --deployment=heapster-v1.2.0
                    - --container=heapster
                    - --poll-period=300000
                    - --estimator=exponential

  - path: /srv/kubernetes/manifests/heapster-svc.yaml
    content: |
        kind: Service
        apiVersion: v1
        metadata: 
          name: heapster
          namespace: kube-system
          labels: 
            kubernetes.io/cluster-service: "true"
            kubernetes.io/name: "Heapster"
        spec: 
          ports: 
            - port: 80
              targetPort: 8082
          selector: 
            k8s-app: heapster

  - path: /srv/kubernetes/manifests/kube-dashboard-rc.yaml
    content: |
        apiVersion: v1
        kind: ReplicationController
        metadata:
          name: kubernetes-dashboard-v1.5.0
          namespace: kube-system
          labels:
            k8s-app: kubernetes-dashboard
            version: v1.5.0
            kubernetes.io/cluster-service: "true"
        spec:
          replicas: 1
          selector:
            k8s-app: kubernetes-dashboard
          template:
            metadata:
              labels:
                k8s-app: kubernetes-dashboard
                version: v1.5.0
                kubernetes.io/cluster-service: "true"
              annotations:
                scheduler.alpha.kubernetes.io/critical-pod: ''
                scheduler.alpha.kubernetes.io/tolerations: '[{"key":"CriticalAddonsOnly", "operator":"Exists"}]'
            spec:
              containers:
              - name: kubernetes-dashboard
                image: gcr.io/google_containers/kubernetes-dashboard-amd64:v1.5.0
                resources:
                  limits:
                    cpu: 100m
                    memory: 50Mi
                  requests:
                    cpu: 100m
                    memory: 50Mi
                ports:
                - containerPort: 9090
                livenessProbe:
                  httpGet:
                    path: /
                    port: 9090
                  initialDelaySeconds: 30
                  timeoutSeconds: 30

  - path: /srv/kubernetes/manifests/kube-dashboard-svc.yaml
    content: |
        apiVersion: v1
        kind: Service
        metadata:
          name: kubernetes-dashboard
          namespace: kube-system
          labels:
            k8s-app: kubernetes-dashboard
            kubernetes.io/cluster-service: "true"
        spec:
          selector:
            k8s-app: kubernetes-dashboard
          ports:
          - port: 80
            targetPort: 9090
        
  - path: /etc/kubernetes/ssl/ca.pem.enc
    encoding: gzip+base64
    content: {{.TLSConfig.CACert}}

  - path: /etc/kubernetes/ssl/apiserver.pem.enc
    encoding: gzip+base64
    content: {{.TLSConfig.APIServerCert}}

  - path: /etc/kubernetes/ssl/apiserver-key.pem.enc
    encoding: gzip+base64
    content: {{.TLSConfig.APIServerKey}}

  - path: /etc/kubernetes/ssl/etcd-client.pem.enc
    encoding: gzip+base64
    content: {{.TLSConfig.EtcdClientCert}}

  - path: /etc/kubernetes/ssl/etcd-client-key.pem.enc
    encoding: gzip+base64
    content: {{.TLSConfig.EtcdClientKey}}


{{ if .UseCalico }}
  - path: /etc/kubernetes/cni/net.d/10-calico.conf
    content: |
        {
            "name": "calico",
            "type": "flannel",
            "delegate": {
                "type": "calico",
                "etcd_endpoints": "{{ .EtcdEndpoints }}",
                "log_level": "none",
                "log_level_stderr": "info",
                "hostname": "$private_ipv4",
                "policy": {
                    "type": "k8s",
                    "k8s_api_root": "http://127.0.0.1:8080/api/v1/"
                }
            }
        }
{{ else }}
  - path: /etc/kubernetes/cni/net.d/10-flannel.conf
    content: |
        {
            "name": "podnet",
            "type": "flannel",
            "delegate": {
                "isDefaultGateway": true
            }
        }
{{ end }}
